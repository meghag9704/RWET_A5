{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5287321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"combined_noDate.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e1926-8b4a-4b1b-a59e-5f056807226f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31877258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tracery in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (0.1.1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "!{sys.executable} -m pip install tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "123c2c72-cfcf-4bf2-b0fb-e587d9047ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9fe47f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: markovify in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (0.9.4)\n",
      "Requirement already satisfied: unidecode in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from markovify) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02d76b5a-4521-4f75-88b1-70195206d56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-md==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: setuptools in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (63.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from tracery.modifiers import base_english\n",
    "\n",
    "\n",
    "import sys\n",
    "!conda install -c conda-forge -y --prefix {sys.prefix} spacy\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_md\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from collections import Counter\n",
    "from textwrap import fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f370bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d2500329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e9670e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (4.27.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a57c6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: packaging in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "236f9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7098db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "96c0ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af418823-a7cd-4aca-bd7e-1ba78d8508df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### manipulating text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12888a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "572e98b9-dc05-455a-bab7-1a1a0a20fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracery breakup: \n",
    "sentences = list(doc.sents)\n",
    "words_t = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d817105",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = markovify.Text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c4479-00cf-4924-87c3-db1340aa3d44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### trying different state sizes. \n",
    "Creating different generators of different state sizes to see what is created. state size 4 and up did not create anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7d00ebe9-fa51-4500-af2a-eb6a4deb5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_1 = markovify.Text(text, state_size=1)\n",
    "gen_2 = markovify.Text(text, state_size=2)\n",
    "gen_3 = markovify.Text(text, state_size=3)\n",
    "gen_4 = markovify.Text(text, state_size=4)\n",
    "gen_5 = markovify.Text(text, state_size=5)\n",
    "gen_6 = markovify.Text(text, state_size=6)\n",
    "gen_7 = markovify.Text(text, state_size=7)\n",
    "gen_8 = markovify.Text(text, state_size=8)\n",
    "gen_9 = markovify.Text(text, state_size=9)\n",
    "gen_10 = markovify.Text(text, state_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b4257630-568f-4ac1-a37d-d5176256b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's what do I wanted to have, I am so noisy sometimes to my soul for pushing yourself and feel defeated.\n",
      "\n",
      "\n",
      "Although not everyone in India yet I was in my head But it wasn’t was it?\n",
      "\n",
      "\n",
      "I need people’s approval when deep down I know I can never go back and I know what this means.\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(gen_1.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_2.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_3.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_4.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_5.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_6.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_7.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_8.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_9.make_sentence())\n",
    "print(\"\\n\")\n",
    "print(gen_10.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922618e-f06c-4b19-bf83-be9175b6b2d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### combining state size 1, 2, and 3\n",
    "This did not work because all generators need the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "02184cd5-31ff-4eea-9bd7-035cffb28202",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All `models` must have the same state size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2n/_0wc0zb96nlb_gcsfwr89gc00000gn/T/ipykernel_46942/770124192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/markovify/utils.py\u001b[0m in \u001b[0;36mcombine\u001b[0;34m(models, weights)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All `models` must have the same state size.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All `models` must have the same state size."
     ]
    }
   ],
   "source": [
    "combo = markovify.combine([gen_1, gen_2, gen_3], [50, 30, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9fe82-d23f-408e-8caf-55151f95a408",
   "metadata": {},
   "source": [
    "### making a short sentence + long sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8ff3b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to go to it.\n",
      "But I will get there.\n",
      "\n",
      "\n",
      "I keep seeing people get under your skin instead?\n",
      "What’s worse when one of those moments.\n",
      "\n",
      "\n",
      "It started with the moon.\n",
      "I’d take more time to ask for help.\n",
      "\n",
      "\n",
      "Maybe because people that do so too.\n",
      "It's because I am going?\n",
      "\n",
      "\n",
      "It was all the same room as you wish with it.\n",
      "And now, I recognize my mom told me I have no ambition Because you are becoming?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    print(generator.make_short_sentence(50))\n",
    "    print(generator.make_sentence())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "46b29f45-ba2c-469b-9269-8acba4c65d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am still mad?\n",
      "But she fell in love with them and enjoy and live my best friend and you became a fly on the roof and ricochets around him.\n",
      "It’s more than I can count I have never actually seen a doctor for it.\n",
      "\n",
      "\n",
      "Being stuck here and then he makes me I have met in the most important to New York, definitely a job market goes well until I love Kendrick and Do they went home has been a good times but it into place that have wished me much goes through the coast line between stones I have realized if I fall day.\n",
      "I feel lonely, I have enough people to like me and the only reason for them Even though I know when I wasn’t smart.\n",
      "It was the last thread, I watched my mother grow frailer by the day.\n",
      "\n",
      "\n",
      "I know where we all day, there is not shut down.\n",
      "I say self diagnosed because I don't want that.\n",
      "On the other 364 days.\n",
      "\n",
      "\n",
      "This past throngs of it.\n",
      "I am in the work to be stuck like she was.\n",
      "I don’t feel the smile.\n",
      "\n",
      "\n",
      "You made myself what would run--faster than they have always felt, That’s a pet of food, buying $18 books, developing film or going wrong?\n",
      "Have you ever just get out of the story which would be on cab fare.\n",
      "It’s more than I can count I have never actually seen a doctor for it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    print(gen_1.make_sentence())\n",
    "    print(gen_2.make_sentence())\n",
    "    print(gen_3.make_short_sentence(70))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f20cb-9a05-4b25-9217-7a1673d99604",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a list with new sentences, using it as a new corpus to create a new generator, establishing rules - lost in translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c993f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are too busy loving others.\n",
      "It's a need to be?\n",
      "0.25 miles into a corner And watch People Because that's what the future holds but I can feel the warmth, Trickling in me tells me that I have lost touch with reality.\n",
      "For the past 4 years I have nothing else to say.\n",
      "She gives you the most - this is why A Little Life impacted me the way white people are — they earn for themselves, not their family.\n",
      "It's because I don’t know what it is.\n",
      "The funny thing is far too much goes wrong.\n",
      "Running away from the skyline, Weekends I get to my life.\n",
      "But its the bad things, the difficult things that are beyond my control.\n",
      "But the boy keeps talking You move on to another one The people you hang out with them, I don't have expectations.\n"
     ]
    }
   ],
   "source": [
    "listNewSent = []\n",
    "for i in range(10):\n",
    "    a = generator.make_sentence()\n",
    "    listNewSent.append(a)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21c17a5f-a5a6-4db6-a611-91af8c632376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are too busy loving others. It's a need to be? 0.25 miles into a corner And watch People Because that's what the future holds but I can feel the warmth, Trickling in me tells me that I have lost touch with reality. For the past 4 years I have nothing else to say. She gives you the most - this is why A Little Life impacted me the way white people are — they earn for themselves, not their family. It's because I don’t know what it is. The funny thing is far too much goes wrong. Running away from the skyline, Weekends I get to my life. But its the bad things, the difficult things that are beyond my control. But the boy keeps talking You move on to another one The people you hang out with them, I don't have expectations.\n",
      "['They', 'are', 'too', 'busy', 'loving', 'others.', \"It's\", 'a', 'need', 'to', 'be?', '0.25', 'miles', 'into', 'a', 'corner', 'And', 'watch', 'People', 'Because', \"that's\", 'what', 'the', 'future', 'holds', 'but', 'I', 'can', 'feel', 'the', 'warmth,', 'Trickling', 'in', 'me', 'tells', 'me', 'that', 'I', 'have', 'lost', 'touch', 'with', 'reality.', 'For', 'the', 'past', '4', 'years', 'I', 'have', 'nothing', 'else', 'to', 'say.', 'She', 'gives', 'you', 'the', 'most', '-', 'this', 'is', 'why', 'A', 'Little', 'Life', 'impacted', 'me', 'the', 'way', 'white', 'people', 'are', '—', 'they', 'earn', 'for', 'themselves,', 'not', 'their', 'family.', \"It's\", 'because', 'I', 'don’t', 'know', 'what', 'it', 'is.', 'The', 'funny', 'thing', 'is', 'far', 'too', 'much', 'goes', 'wrong.', 'Running', 'away', 'from', 'the', 'skyline,', 'Weekends', 'I', 'get', 'to', 'my', 'life.', 'But', 'its', 'the', 'bad', 'things,', 'the', 'difficult', 'things', 'that', 'are', 'beyond', 'my', 'control.', 'But', 'the', 'boy', 'keeps', 'talking', 'You', 'move', 'on', 'to', 'another', 'one', 'The', 'people', 'you', 'hang', 'out', 'with', 'them,', 'I', \"don't\", 'have', 'expectations.']\n"
     ]
    }
   ],
   "source": [
    "newText = ' '.join(listNewSent)\n",
    "print(newText)\n",
    "newWords = newText.split()\n",
    "print(newWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f1eb62e-5f59-43e5-99a3-62f718d7bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8a685dec-c657-4a9f-ba0d-cc23ad298dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from initial document - spacy\n",
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "subjects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "objects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'dobj']\n",
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "past_tense_verbs = [w for w in words if w.tag_ == 'VBD']\n",
    "adjs = [w for w in words if w.tag_ == \"JJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]\n",
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5fbe5dbe-c74a-48b0-a4c0-0c0776ba4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rules = {\n",
    "    \"sentences\": [w.text for w in sentences],\n",
    "    \"word\": [w.text for w in words],\n",
    "    \"subject\": [w.text for w in subjects],\n",
    "    \"object\": [w.text for w in objects],\n",
    "    \"verb\": [w.text for w in verbs], # exclude common irregular verbs\n",
    "    \"adj\": [w.text for w in adjs],\n",
    "    \"entities\": [w.text for w in entities],\n",
    "    \"noun\": [w.text for w in nouns],\n",
    "    \"adv\": [w.text for w in advs],\n",
    "    \"people\": [w.text for w in people],\n",
    "    \"location\": [w.text for w in locations], \n",
    "    \"time\": [w.text for w in times],\n",
    "    \"origin\": \"In the midst of it all -- there will always be #people.a# who will say -- \\\"#object# #verb# #adj# #noun#.\\\" \"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3ac8fa5a-1640-4c97-9abc-ff97d2541102",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grammar = tracery.Grammar(final_rules)\n",
    "final_grammar.add_modifiers(base_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d1c672dc-ecb8-4eca-884a-d00b9ba862a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1:42, tonight, 3am, the oddest hours of the night, 12am, night, Last night, hours, 9pm - I, This morning, 3:55 AM, 4 am, 20 minutes ago, a couple of minutes, every night, the next morning, 4am turn, 8am, 24 hours, afternoon, one fleeting minute, Hour after hour, the hour, that night, evening, seconds, 2:42 in the morning, hours, Saturday nights, 4 am, a few hours, a night, odd hours, Every night, a minute, all night, 10pm]\n"
     ]
    }
   ],
   "source": [
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "434aa209-0191-4b78-b507-5b8278f2d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_1 = {\n",
    "    \"subject\": [w.text for w in subjects2],\n",
    "    \"object\": [w.text for w in objects2],\n",
    "    \"verb\": [w.text for w in verbs2], # exclude common irregular verbs\n",
    "    \"adj\": [w.text for w in adjs2],\n",
    "    \"entities\": [w.text for w in entities2],\n",
    "    \"noun\": [w.text for w in nouns2],\n",
    "    \"adv\": [w.text for w in advs2],\n",
    "    \"people\": [w.text for w in people],\n",
    "    \"location\": [w.text for w in locations], \n",
    "    \"time\": [w.text for w in times],\n",
    "    \"origin\": \"It is what #people# say -- \\\"#object# #verb# #adj# #noun#.\\\" \"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9bbc49b3-fe9e-4706-aeb1-6ddad913ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = tracery.Grammar(r_1)\n",
    "grammar.add_modifiers(base_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b56e91b8-941d-4f2d-b27e-4393c2156fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is what Yknow say -- \"what have past others.\" I can be loved that everything good times, when I was craving.\n",
      "\n",
      "It is what Diamond say -- \"others talking much warmth.\" That's the way to her and such.\n",
      "\n",
      "It is what Jenny say -- \"nothing have busy thing.\" That’s how life has been so alone.\n",
      "\n",
      "It is what Yknow say -- \"People hang wrong thing.\" I try to my strength.\n",
      "\n",
      "It is what Van Gogh say -- \"The people holds funny things.\" It makes sense really, how life crisis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(grammar.flatten(\"#origin#\") + gen_1.make_short_sentence(100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "380b9fe3-1931-4f14-9977-f0a8d95ccfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But then he hurts not thinking about the window pane.Because I want this to be wanted.Because I am not worthy.\n",
      "That’s a while, you have been 4 years ago, I have watched Troye Sivan live with this.I am too lazy to text them and keep them company.Because I have a driver.\n",
      "But you love?And I am ready to parade in.It’s more than I should?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2n/_0wc0zb96nlb_gcsfwr89gc00000gn/T/ipykernel_46942/2764062681.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_short_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgen_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_short_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgen_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_short_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    print(gen_1.make_short_sentence(100) + gen_2.make_short_sentence(50) + gen_3.make_short_sentence(25)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b78301-c37e-4662-b93f-8d31c7f5ac89",
   "metadata": {},
   "source": [
    "# Shifting to large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69abb2-6acf-4674-a868-3b7ee9abdd89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### importing GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2dbd820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "llm_model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a0be6908-9f2e-45dc-b95d-57b656c582fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "716c4b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c308daae652c4330b73d9bd619e2a3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349e850b10034fa485c05b46c89a7367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701bc224f6ea44a08e5cdf9e6eaf5407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efd3977302942ca82f16073b08b8c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d00b6be69842d4b6bbad3178f3cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c18510b43a04cb8aa3a2d63b8c57ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_gen = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8bf7600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(20)\n",
    "llm_vocab = llm_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15ac2f-3255-403b-acaf-9dec1bc978a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### creating sentence with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "768fb5ff-ab7c-43ce-97a5-ef2a92fdcde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Silence is icky and doesn't work.\"},\n",
       " {'generated_text': 'Silence is ____________(This is an'},\n",
       " {'generated_text': 'Silence is _______ | (L,L'},\n",
       " {'generated_text': 'Silence is \\xa0an act of rebellion,'},\n",
       " {'generated_text': 'Silence is \")\\n\\nOn the other'}]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gen(\"Silence is \", max_length=10, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade5a5b-6b5e-4b05-bfb7-5b215fd8ef11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### training a GPT2 model on original pieces of writing - leading to a not very well trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a262eb-0ab8-4f30-9f8d-ee8420bcedc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### returning to text generation with original llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c9cfacbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow stripe from the other end of the road, and were almost identical to those found on the other side. The car had a front end that rotated just after hitting the intersection, and a rear end that rotated almost every time the drivers crossed the median.\\n\\nPolice were called to the accident at about 11:30 p.m. and called to report an accident in the 500 block of Washington Street. The vehicle was described as a 2015 Ford Bronco, likely'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gen(\"Two roads diverged in a yellow\", max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "55359445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Silence is nothing to be ashamed of but is essential to human life, not something to be ashamed of but is essential to human life, and there are many ways to deal with the difficulties that have arisen in such a way even as we are currently experiencing. It is important to consider the difficulties that face us as we continue to work toward our goal of becoming the most technologically advanced nation on Earth. It is important to understand that for most of our history there was no such thing as a standard of'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_gen(\"Silence is\", max_length=100)[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b177a-5ee8-4464-8c9e-158dafcfeaf1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### brining in the trained LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c74821fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grimms_tokenizer = AutoTokenizer.from_pretrained('grimms_gpt2')\n",
    "grimms_model = AutoModelForCausalLM.from_pretrained('grimms_gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "77e51fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedLLM_generator = pipeline(\"text-generation\", model=trainedLLM_model, tokenizer=trainedLLM_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "43a96fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/meghagoel/opt/anaconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow line above the street, and before the intersection of North and Northeast Avenue, came a red line extending down North Street. I parked at my front door. On the other side of the green line was a red line'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedLLM_generator(\"Two roads diverged in a yellow\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4879a21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Silence is just a buzzword.\"\\n\\nThe latest round has been filled with allegations surrounding allegations of corruption. One of the parties involved in the talks, who did not want to be identified, says he wanted to protect the country\\'s independence.'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedLLM_generator(\"Silence is\")[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c791631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Silence is coming to Adelaide! Scrimshaw is coming up.\\n\\n\\nFind Scrimshaw only on Facebook!\\n\\nRomance in Images\\n\\nIn Culture, a novel by Jelulia Nys, the teenager conceives of her life as the ultimate vacillator and wanders answerably out of touch with the human soul, who always really wants to find her own. The book is set in a very different world, where, in a world filled with different people,'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedLLM_generator(\"Silence is\",\n",
    "          top_k=trainedLLM_tokenizer.vocab_size,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9f489-ed7c-478e-8bfd-b9ca2c4e86ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### training a GPT2 model on grimm's fairytales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7577847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/meghagoel/.cache/huggingface/datasets/text/default-2f3176d17f885dab/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4c81c514e947dabe30534d4fbd15ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665b3a3973d042718301aedd780f4dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/meghagoel/.cache/huggingface/datasets/text/default-2f3176d17f885dab/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2a7ce7474143659b1381b07038d16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data_grimms = datasets.load_dataset('text', data_files=\"grimms.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e6256d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "tokenized_training_data = training_data_grimms.map(\n",
    "    lambda x: llm_tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "585664a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 64\n",
    "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b5ab9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_trainer = Trainer(model=llm_model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='grimms_gpt2',\n",
    "                      num_train_epochs=1,\n",
    "                      do_train=True,\n",
    "                      do_eval=False\n",
    "                  ),\n",
    "                  tokenizer=llm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b8e83df4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2n/_0wc0zb96nlb_gcsfwr89gc00000gn/T/ipykernel_46942/2209585537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2691\u001b[0m                     \u001b[0;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m                     \u001b[0;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: last_hidden_state,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "llm_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "25b6dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c16c4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grimms_tokenizer = AutoTokenizer.from_pretrained('grimms_gpt2')\n",
    "grimms_model = AutoModelForCausalLM.from_pretrained('grimms_gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e31bad2-9656-4b4c-84ef-f0ca095da733",
   "metadata": {},
   "outputs": [],
   "source": [
    "grimms_generator = pipeline(\"text-generation\", model=grimms_model, tokenizer=grimms_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5d8adcc9-c857-48df-a546-a4e7aa521fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellowish light and an featherset appeared. It was a woman, clad in silk, holding a long dagger in one hand and a dagger in the other. It led them right over the threshold towards the river, where'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grimms_generator(\"Two roads diverged in a yellow\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6be02b6f-8d8e-43a5-8993-6d949c4a4a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Silence is coming!\"\\n\\n\"Come here!\" Ruby, with a worried look on her face, gave it two very big smiles. \"It\\'s time to go see the castle. I will definitely help you with anything you need for the rest'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grimms_generator(\"Silence is\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95adb941-2ddc-47e4-b58b-0b6c38a6c90b",
   "metadata": {},
   "source": [
    "### Getting bored with an LLM model leading to trying to use a markov generated sentence to use on a gpt2 model trained on grimms fairytales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "647a402f-f308-4597-a9b8-d60bdb597b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your problem is that people are there in a different car from everyone practically • The friends are the last 6 months thanks to the magnet wasn't strong enough for me.\n"
     ]
    }
   ],
   "source": [
    "currentSentence = generator.make_sentence()\n",
    "print(currentSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "fa54ffdd-be45-41d6-94eb-510e10df58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your problem is that people are there in a different car from everyone practically • The friends are the last 6 months thanks to the magnet wasn't strong enough for me. The magnet didn't work and when I tried setting up for him I felt like an\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grimms_generator(currentSentence)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59999588-5533-47d0-a9a2-71b1483fcb50",
   "metadata": {},
   "source": [
    "**Trying to loop a sentence through grimm generator. \n",
    "Markov --> grimms --> grimms --> grimms --> grimms --> grimms (finaloutput).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "b55cdd13-e11e-44a0-a6c0-47f963e7650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every time something bad happen to me. You do not need to be worried about that.\"\n",
      "\n",
      "Garry Dank, 37, took issue with the lack of attention to information in his life. \"I need to be happy before I kill myself,\" he said. \"I know I am too young, and I know that to kill myself after I kill myself is not the right thing to do. I feel shame. I feel that if I could just kill my own family I could just have\n"
     ]
    }
   ],
   "source": [
    "curr = generator.make_sentence()\n",
    "for i in range(5): \n",
    "    n = grimms_generator(curr, max_length=100)[0]['generated_text']\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3c16649f-fabe-4a37-94bd-635d020c0bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 101, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 102, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 103, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, This freedom of walking in this rain, _______________ Flying over the horizon, a light pinkish hue as it stood tall and proud yet shyly peeking through the assumptions. This, This time, _______________ I saw _______________ at the peak of this day. This, This, This. This, _______________ And this, This, _______________ On the way to the garden _______ of his house. _______________ I saw _______________ in the green\n"
     ]
    }
   ],
   "source": [
    "curr = generator.make_sentence()\n",
    "for i in range(5): \n",
    "    curr = grimms_generator(curr, max_length=100)[0]['generated_text']\n",
    "print(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "695e1c46-f704-4f84-b887-c4e1478f33fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is that too much sometimes I need a lot of things that aren’t true — fallacies of self hate.\n",
      " \n",
      "Is that too much sometimes I need a lot of things that aren’t true — fallacies of self hate.‑— (@PunisherZilker) December 5, 2017\n",
      "\n",
      "For now he only does what is acceptable to himself, even if there is a threat to the community or an attack in thatipples of racism and self-hatred.\n",
      "\n",
      "[This post has been updated in 2018]\n"
     ]
    }
   ],
   "source": [
    "a = generator.make_sentence()\n",
    "curr = a\n",
    "for i in range(5): \n",
    "    curr = grimms_generator(curr, max_length=100)[0]['generated_text']\n",
    "print(a + \"\\n \\n\" + curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ea738a56-182a-4e09-8372-6cd9ecf438b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because throwing sticks at you, I keep expecting? (Cough…I need to… I need not Stefano back to our day! Hey, Stefano!) Hey! Yours just a girl!\"\n",
      "\n",
      "\"Ow-!\"\n",
      "\n",
      "\n",
      "\"And her voice just growls. \"Ow!\" I try not to sound so worried, because she said something like that… I'm scared… My friend's got her now. Her voice is like a fairy tale. Something's happening in her head… her words… all those weird things her… her life is going to give you… It's just hard to believe she got off that boat she's on.\"\n",
      "\n",
      "Mouth got out of her mouth as if I was seeing things. \"And it says she's back.\"\n",
      "\n",
      "\"Oh. Oh, so… it's just impossible now. It seems like a day has passed. My hands in mine, just hold her for no good reason. Do you?\"\n",
      "\n",
      "\n",
      "\"Come, Stefano. I have to go get her, don't I? She's… weird… She's… her mom didn't want me taking her away. I'm not gonna put a face on it. She'll take the phone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curr = generator.make_sentence()\n",
    "lenUpdate = 50\n",
    "for i in range(5): \n",
    "    curr = grimms_generator(curr, max_length=lenUpdate)[0]['generated_text']\n",
    "    lenUpdate = lenUpdate + 50\n",
    "print(curr + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcb882-efb9-4fbc-b614-61e31b520271",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d9950bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from initial document - spacy\n",
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "subjects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'nsubj']\n",
    "objects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'dobj']\n",
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "past_tense_verbs = [w for w in words if w.tag_ == 'VBD']\n",
    "adjs = [w for w in words if w.tag_ == \"JJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]\n",
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "61b6517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rules = {\n",
    "    \"sentences\": [w.text for w in sentences],\n",
    "    \"word\": [w.text for w in words],\n",
    "    \"subject\": [w.text for w in subjects],\n",
    "    \"object\": [w.text for w in objects],\n",
    "    \"verb\": [w.text for w in verbs], # exclude common irregular verbs\n",
    "    \"adj\": [w.text for w in adjs],\n",
    "    \"entities\": [w.text for w in entities],\n",
    "    \"noun\": [w.text for w in nouns],\n",
    "    \"adv\": [w.text for w in advs],\n",
    "    \"people\": [w.text for w in people],\n",
    "    \"location\": [w.text for w in locations], \n",
    "    \"time\": [w.text for w in times],\n",
    "    \"origin\": \"In the midst of it all -- there will always be #people.a# who will say -- \\\"#object# #verb# #adj# #noun#.\\\" \"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2c142d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grammar = tracery.Grammar(final_rules)\n",
    "final_grammar.add_modifiers(base_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ac9c921e-7209-4c6f-aa01-e035610ca251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the midst of it all -- there will always be a Beer who will say -- \"what ashaamed main description.\" \n",
      "\n",
      "In the midst of it all -- there will always be a Jen who will say -- \"myself did unrealistic people.\" \n",
      "\n",
      "In the midst of it all -- there will always be a Jen who will say -- \"it facilitates trodden oysters.\" \n",
      "\n",
      "In the midst of it all -- there will always be a Van Gogh who will say -- \"life hope first head.\" \n",
      "\n",
      "In the midst of it all -- there will always be an Alice who will say -- \"this idea know rare judgment.\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(final_grammar.flatten(\"#origin#\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "898d7101-cacd-4b3a-9da1-f830594c33e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the midst of it all -- there will always be a maa who will say -- \"me make complete society.\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the midst of it all -- there will always be a Kaavya who will say -- \"me walked peripheral Moments.\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the midst of it all -- there will always be a Jenny who will say -- \"pick-me-ups happen different exam.\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the midst of it all -- there will always be a Dadi who will say -- \"history call past People.\" \n",
      "In the midst of it all -- there will always be an Instagram who will say -- \"it love bad class.\" \n",
      "In the midst of it all --You treat me like a thunderstorm is coming, Just so I could have ever asked and it was bad.I feel pain in my heart.But the pain is not for me --My pain is for you. I know this.I know what it is like for you right here, Just you guys you gotta understand and I try everything I can to help you. But I have a message that will be in your heart. It's true, this was part of me when I was a child. It means a lot to me now.I mean. It's an all the life you've experienced. If you were born this way then it all will hurt, it won't do anything to change the way you are.But I'm not gonna try any more you know? They make me miserable, You'll get better.But I'll be there when I do finally reach out. I've been for years, and I'm not gonna let up until then and I'll be there for you.I'll say my prayers and my prayer. I know we have some issues here but we're gonna come together, we love you.We believe in each other.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "curr = 'In the midst of it all --' + generator.make_sentence()\n",
    "lenUpdate = 50\n",
    "for i in range(5): \n",
    "    curr = grimms_generator(curr, max_length=lenUpdate)[0]['generated_text']\n",
    "    lenUpdate = lenUpdate + 50\n",
    "    print(final_grammar.flatten(\"#origin#\"))\n",
    "print(curr + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8def4-9129-46f5-b7bf-417d089408da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
